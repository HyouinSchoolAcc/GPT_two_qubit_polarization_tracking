{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06089fc4-b7f7-42f0-990a-f4fa1028a318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of           TimeStamps       AI0       AI1       AI2       AI3       AI4  \\\n",
      "0           0.000000  2.229548  0.295483 -1.081807  1.623932 -1.120879   \n",
      "1           0.101122  2.219780  0.295483 -1.101343  1.633699 -1.101343   \n",
      "2           0.203640  2.210012  0.285714 -1.081807  1.643468 -1.081807   \n",
      "3           0.300221  2.219780  0.305250 -1.101343  1.653235 -1.072039   \n",
      "4           0.403315  2.200244  0.315019 -1.101343  1.653235 -1.062271   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "711710  71171.002492 -0.964591 -2.029304 -1.062271 -0.925519  0.686203   \n",
      "711711  71171.100106 -0.876678 -2.078144 -1.013431 -0.915751  0.676435   \n",
      "711712  71171.201757 -0.759462 -2.136752 -0.993895 -0.896214  0.637362   \n",
      "711713  71171.303305 -0.769231 -2.126984 -1.032967 -0.827839  0.656898   \n",
      "711714  71171.401576 -0.769231 -2.146520 -0.896214 -0.905983  0.520146   \n",
      "\n",
      "             AI5  \n",
      "0      -1.492064  \n",
      "1      -1.501831  \n",
      "2      -1.501831  \n",
      "3      -1.492064  \n",
      "4      -1.492064  \n",
      "...          ...  \n",
      "711710 -2.185592  \n",
      "711711 -2.185592  \n",
      "711712 -2.214896  \n",
      "711713 -2.205128  \n",
      "711714 -2.214896  \n",
      "\n",
      "[711715 rows x 7 columns]>\n",
      "step 0: train loss 1.0962, val loss 2.0770\n",
      "step 1: train loss 0.6674, val loss 1.7501\n",
      "step 2: train loss 0.6385, val loss 1.7039\n",
      "step 3: train loss 0.7797, val loss 1.4933\n",
      "step 4: train loss 0.7114, val loss 1.2678\n",
      "step 5: train loss 0.5130, val loss 0.9263\n",
      "step 6: train loss 0.3390, val loss 0.7435\n",
      "step 7: train loss 0.2785, val loss 0.5596\n",
      "step 8: train loss 0.3084, val loss 0.4321\n",
      "step 9: train loss 0.3859, val loss 0.2899\n",
      "step 10: train loss 0.4667, val loss 0.2770\n",
      "step 11: train loss 0.4878, val loss 0.3243\n",
      "step 12: train loss 0.4126, val loss 0.3059\n",
      "step 13: train loss 0.3513, val loss 0.2623\n",
      "step 14: train loss 0.3086, val loss 0.2348\n",
      "step 15: train loss 0.2576, val loss 0.1960\n",
      "step 16: train loss 0.2418, val loss 0.1958\n",
      "step 17: train loss 0.2411, val loss 0.1981\n",
      "step 18: train loss 0.2419, val loss 0.2322\n",
      "step 19: train loss 0.2526, val loss 0.2566\n"
     ]
    }
   ],
   "source": [
    "#import some stuff\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import math\n",
    "from scipy.optimize import minimize\n",
    "from scipy.interpolate import CubicSpline\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gc\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def clean_memory():\n",
    "    gc.collect()           # Garbage collect Python objects\n",
    "    torch.cuda.empty_cache()  # Clear cached memory on GPU\n",
    "\n",
    "\n",
    "data_dir = \"C:/Users/yueze/Desktop/GPT_two_qubit_polarization_tracking/20240506_2131.csv\"\n",
    "df = pd.read_csv(data_dir)\n",
    "print(df.head)\n",
    "\n",
    "features = [\"AI0\",\"AI1\",\"AI2\"]\n",
    "targets  = [\"AI3\",\"AI4\",\"AI5\"]\n",
    "x = df[features]  # Dropping original targets as we'll use aligned targets\n",
    "y = df[targets]  # Using aligned targets\n",
    "split_idx = int(len(df) * 0.9)\n",
    "length_read=len(df)\n",
    "# Split into training and testing sets\n",
    "x_train = x.iloc[:split_idx]\n",
    "y_train = y.iloc[:split_idx]\n",
    "x_test = x.iloc[split_idx:]\n",
    "y_test = y.iloc[split_idx:]\n",
    "\n",
    "input_feature_dim = 3  # Each input element is a 1x3 vector\n",
    "embed_size = 64\n",
    "target_dim = 3\n",
    "block_size = 40\n",
    "num_heads = 16\n",
    "max_iters = 144\n",
    "batch_size = 10\n",
    "eval_iters = 200\n",
    "eval_interval = 1\n",
    "num_layers=5\n",
    "\n",
    "def get_batch3(split):\n",
    "    # Select the correct data split\n",
    "    if split == 'train':\n",
    "        a, b, max_index = x_train, y_train, int(length_read * 0.9) - block_size - 1\n",
    "    else:  # split == 'test'\n",
    "        a, b, max_index = x_test, y_test, length_read - (int(length_read * 0.9) + block_size + 1)\n",
    "\n",
    "    # Generate random indices for batch selection, ensuring they're within bounds\n",
    "    ix = torch.randint(0, max_index, (batch_size,))\n",
    "\n",
    "    # Initialize lists to hold the batches\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    for i in ix:\n",
    "        try:\n",
    "                        # Extract the sequence from 'a' and the corresponding target from 'b'\n",
    "            seq = torch.tensor(a.iloc[i.item():i.item() + block_size].astype(np.float32).values, dtype=torch.float32)\n",
    "            target = torch.tensor(b.iloc[i.item() + block_size].astype(np.float32).values, dtype=torch.float32)\n",
    "\n",
    "            x_batch.append(seq)\n",
    "            y_batch.append(target)\n",
    "        except IndexError as e:\n",
    "            print(f\"IndexError for index {i.item()}: {str(e)}\")\n",
    "            print(f\"Attempting to access index [{i.item()}:{i.item() + block_size}] in 'a' with shape {a.shape}\")\n",
    "            print(f\"Attempting to access index {i.item() + block_size} in 'b' with shape {b.shape}\")\n",
    "            # Optionally, break or continue depending on desired behavior on error\n",
    "            break  # or continue\n",
    "\n",
    "    if not x_batch or not y_batch:\n",
    "        print(\"Error: Batch could not be created due to index issues.\")\n",
    "        return None, None\n",
    "\n",
    "    # Stack the collected sequences and targets into tensors\n",
    "    xstack = torch.stack(x_batch)\n",
    "    ystack = torch.stack(y_batch)\n",
    "\n",
    "    return xstack, ystack\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.keys = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.queries = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.values = nn.Linear(embed_size, embed_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        K = self.keys(x)\n",
    "        Q = self.queries(x)\n",
    "        V = self.values(x)\n",
    "\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.embed_size ** 0.5\n",
    "        attention = torch.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        attended = torch.matmul(attention, V)\n",
    "        return attended\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert embed_size % num_heads == 0\n",
    "\n",
    "        self.head_dim = embed_size // num_heads\n",
    "\n",
    "        self.keys = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.queries = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.values = nn.Linear(embed_size, embed_size, bias=False)\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _ = x.shape\n",
    "\n",
    "        keys = self.keys(x).view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        queries = self.queries(x).view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        values = self.values(x).view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "\n",
    "        attention_scores = torch.einsum(\"bnqh,bnkh->bnqk\", [queries, keys]) / (self.head_dim ** 0.5)\n",
    "        attention = torch.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        attended = torch.einsum(\"bnqk,bnkv->bnqv\", [attention, values]).reshape(batch_size, seq_length, self.embed_size)\n",
    "\n",
    "        output = self.fc_out(attended)\n",
    "        return output\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch3(split)\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train\n",
    "    del X, Y\n",
    "    return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.attention = MultiHeadAttention(embed_size, num_heads)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, 2 * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * embed_size, embed_size),\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, value):\n",
    "        x = self.norm1(value)\n",
    "        attention_output = self.attention(x)\n",
    "        x = value + self.dropout1(attention_output)  # Residual connection and dropout after attention\n",
    "        x = self.norm2(x)\n",
    "        feed_forward_output = self.feed_forward(x)\n",
    "        out = value + self.dropout2(feed_forward_output)  # Residual connection and dropout after FFN\n",
    "        return out\n",
    "\n",
    "# Positional Encoding in Encoder class should be moved to the device\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_feature_dim, embed_size, num_heads, num_layers, seq_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_fc = nn.Linear(input_feature_dim, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, seq_length, embed_size)).to(device)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(embed_size, num_heads) for _ in range(num_layers)])\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.input_fc(x)) + self.positional_encoding\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def to_cpu(self):\n",
    "        # Move the entire model to CPU\n",
    "        self.input_fc.to('cpu')\n",
    "        self.positional_encoding.data = self.positional_encoding.data.cpu()\n",
    "        for layer in self.layers:\n",
    "            layer.to('cpu')\n",
    "        self.relu.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "class EncoderDecoderModelWithMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_feature_dim, embed_size, target_dim, seq_length, num_heads, num_layers):\n",
    "        super(EncoderDecoderModelWithMultiHeadAttention, self).__init__()\n",
    "        self.encoder = Encoder(input_feature_dim, embed_size, num_heads, num_layers, seq_length)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embed_size, target_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        encoded = self.encoder(x)\n",
    "        encoded_pooled = torch.mean(encoded, dim=1)\n",
    "        decoded = self.decoder(encoded_pooled)\n",
    "\n",
    "        if targets is not None:\n",
    "            loss = criterion(decoded, targets)\n",
    "            return decoded, loss\n",
    "\n",
    "\n",
    "        return decoded, None\n",
    "\n",
    "    def to_cpu(self):\n",
    "        self.encoder.to_cpu()\n",
    "        for layer in self.decoder:\n",
    "            layer.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "actuals = []\n",
    "predictions = []\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = EncoderDecoderModelWithMultiHeadAttention(input_feature_dim, embed_size, target_dim, block_size, num_heads, num_layers).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "        # Evaluate the loss periodically\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        xb, yb = get_batch3('train')\n",
    "        xb, yb = xb.to(device), yb.to(device)  # Ensure these tensors are on the correct device\n",
    "\n",
    "        predictions, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                # After using tensors in a training step\n",
    "        del xb, yb, predictions  # Assuming these are not needed after the training step\n",
    "        clean_memory()  # Call this to clear memory\n",
    "    print(\"Loss:\", loss.item())\n",
    "    del loss  # Assuming these are not needed after the training step\n",
    "    clean_memory()  # Call this to clear memoryq\n",
    "    model.to_cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be478e56-6e44-4e94-82ad-f7dd6bf42988",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df[\"AI3\"][480460:480800])\n",
    "plt.title('Feature AI3')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe13bcd8-8eda-4edd-b30d-a9540a52ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1, these ranges are :\n",
    "#:500-670\n",
    "#463000:485000;465950:466150;480460:480800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46a1318-f4a6-4800-9379-bdb3d054d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the indices for train_x\n",
    "train_indices = list(range(500, 670)) + list(range(465950, 466150)) + list(range(480460, 480800))\n",
    "\n",
    "# Features and targets\n",
    "features = [\"AI0\", \"AI1\", \"AI2\"]\n",
    "targets = [\"AI3\", \"AI4\", \"AI5\"]\n",
    "\n",
    "# Extract x and y from the dataframe\n",
    "x = df[features]\n",
    "y = df[targets]\n",
    "\n",
    "# Create the training sets using the specified indices\n",
    "x_train = x.iloc[train_indices]\n",
    "y_train = y.iloc[train_indices]\n",
    "\n",
    "# Use the remaining data for testing\n",
    "test_indices = set(df.index) - set(train_indices)\n",
    "x_test = x.loc[test_indices]\n",
    "y_test = y.loc[test_indices]\n",
    "\n",
    "# Display the lengths of the training and testing sets\n",
    "print(f'Length of training set: {len(x_train)}')\n",
    "print(f'Length of testing set: {len(x_test)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1ee475-e7de-4b22-8862-bcf67817dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actuals = []\n",
    "predictions = []\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = EncoderDecoderModelWithMultiHeadAttention(input_feature_dim, embed_size, target_dim, block_size, num_heads, num_layers).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "        # Evaluate the loss periodically\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        xb, yb = get_batch3('train')\n",
    "        xb, yb = xb.to(device), yb.to(device)  # Ensure these tensors are on the correct device\n",
    "\n",
    "        predictions, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                # After using tensors in a training step\n",
    "        del xb, yb, predictions  # Assuming these are not needed after the training step\n",
    "        clean_memory()  # Call this to clear memory\n",
    "    print(\"Loss:\", loss.item())\n",
    "    del loss  # Assuming these are not needed after the training step\n",
    "    clean_memory()  # Call this to clear memoryq\n",
    "    model.to_cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45586ec2-c76b-474e-aa8e-e598bd656029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
