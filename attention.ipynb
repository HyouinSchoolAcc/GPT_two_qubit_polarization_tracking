{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a48639f-65ae-429f-838d-84f6f7aade6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bed28369-aa62-4a90-a1b1-42db3990c951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#things to note: The sample rate is quite high. If it becomes computationally expensive\n",
    "#we can always dumb down the sampling rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a293990-1a44-4a53-9c6b-d04ad1b4858a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length of the dataset is: 12546118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yueze\\AppData\\Local\\Temp\\ipykernel_26244\\2852126022.py:13: DtypeWarning: Columns (0,1,2,3,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"./20240229_2308.csv\", header=None).iloc[1:length_read, :6]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l1</th>\n",
       "      <th>l2</th>\n",
       "      <th>l3</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.390720118768513</td>\n",
       "      <td>-0.5347981965169311</td>\n",
       "      <td>-0.16361381206661463</td>\n",
       "      <td>0.9987793918699026</td>\n",
       "      <td>-0.13430978171527386</td>\n",
       "      <td>-2.253967977128923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.400488128885627</td>\n",
       "      <td>-0.5347981965169311</td>\n",
       "      <td>-0.16361381206661463</td>\n",
       "      <td>1.0085474019870162</td>\n",
       "      <td>-0.15384580194950104</td>\n",
       "      <td>-2.2637359872460365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.400488128885627</td>\n",
       "      <td>-0.5347981965169311</td>\n",
       "      <td>-0.16361381206661463</td>\n",
       "      <td>1.0085474019870162</td>\n",
       "      <td>-0.15384580194950104</td>\n",
       "      <td>-2.2637359872460365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.390720118768513</td>\n",
       "      <td>-0.5347981965169311</td>\n",
       "      <td>-0.16361381206661463</td>\n",
       "      <td>0.9987793918699026</td>\n",
       "      <td>-0.13430978171527386</td>\n",
       "      <td>-2.2637359872460365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-2.400488128885627</td>\n",
       "      <td>-0.5347981965169311</td>\n",
       "      <td>-0.17338182218372822</td>\n",
       "      <td>1.0085474019870162</td>\n",
       "      <td>-0.13430978171527386</td>\n",
       "      <td>-2.2637359872460365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   l1                   l2                    l3  \\\n",
       "1  -2.390720118768513  -0.5347981965169311  -0.16361381206661463   \n",
       "2  -2.400488128885627  -0.5347981965169311  -0.16361381206661463   \n",
       "3  -2.400488128885627  -0.5347981965169311  -0.16361381206661463   \n",
       "4  -2.390720118768513  -0.5347981965169311  -0.16361381206661463   \n",
       "5  -2.400488128885627  -0.5347981965169311  -0.17338182218372822   \n",
       "\n",
       "                   r1                    r2                   r3  \n",
       "1  0.9987793918699026  -0.13430978171527386   -2.253967977128923  \n",
       "2  1.0085474019870162  -0.15384580194950104  -2.2637359872460365  \n",
       "3  1.0085474019870162  -0.15384580194950104  -2.2637359872460365  \n",
       "4  0.9987793918699026  -0.13430978171527386  -2.2637359872460365  \n",
       "5  1.0085474019870162  -0.13430978171527386  -2.2637359872460365  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full = pd.read_csv(\"./20240229_2308.csv\", header=None, engine='python')\n",
    "\n",
    "# The maximum length of the dataset is the number of rows\n",
    "max_length = df_full.shape[0]\n",
    "\n",
    "# Print the maximum length\n",
    "print(f\"The maximum length of the dataset is: {max_length}\")\n",
    "\n",
    "# Now, you can use this information to set length_read appropriately\n",
    "length_read = min(12546000, max_length)  # for example, to limit it to 100000 or the maximum length available\n",
    "\n",
    "# Proceed with loading the desired portion of the dataset\n",
    "df = pd.read_csv(\"./20240229_2308.csv\", header=None).iloc[1:length_read, :6]\n",
    "df.columns = [\"l1\",\"l2\",\"l3\",\"r1\",\"r2\",\"r3\"]\n",
    "df.head()\n",
    "\n",
    "#the maximum length is 12546118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f90cd419-f29b-4123-bb2e-0b5bc21d8e08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12546118\n"
     ]
    }
   ],
   "source": [
    "print (max_length)\n",
    "eval_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ef800bb-8087-467f-8df2-1e540fbaea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\"r1\", \"r2\", \"r3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4627310-2c5c-4d82-9e51-cf91282d06af",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(columns=targets)\n",
    "y = df[targets]\n",
    "split_idx = int(len(df) * 0.9)\n",
    "\n",
    "# Split into training and testing sets\n",
    "x_train = x.iloc[:split_idx]\n",
    "y_train = y.iloc[:split_idx]\n",
    "x_test = x.iloc[split_idx:]\n",
    "y_test = y.iloc[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42d6bc87-f700-4f8e-89e7-5f69be064557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif __name__ == \"__main__\":\\n    input_feature_dim = 3  # Each input element is a 1x3 vector\\n    embed_size = 32\\n    target_dim = 3\\n    seq_length = 6\\n    num_heads = 8\\n\\n    model = EncoderDecoderModelWithMultiHeadAttention(input_feature_dim, embed_size, target_dim, seq_length, num_heads)\\n    \\n    # Example input (batch_size, seq_length, input_feature_dim)\\n    x = torch.rand(4, seq_length, input_feature_dim)  # Batch size of 4\\n    targets = torch.rand(4, target_dim)  # Corresponding targets\\n\\n    # Forward pass\\n    predictions = model(x)\\n    \\n    # Example training components\\n    criterion = nn.MSELoss()\\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\\n    \\n    # Compute loss\\n    loss = criterion(predictions, targets)\\n    \\n    # Backpropagation\\n    optimizer.zero_grad()\\n    loss.backward()\\n    optimizer.step()\\n    \\n    print(\"Loss:\", loss.item())\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "input_feature_dim = 3  # Each input element is a 1x3 vector\n",
    "embed_size = 64\n",
    "target_dim = 3\n",
    "block_size = 8\n",
    "num_heads = 16\n",
    "max_iters = 1000\n",
    "batch_size = 10\n",
    "\n",
    "\n",
    "\n",
    "def get_batch3(split):\n",
    "    # Select the correct data split\n",
    "    if split == 'train':\n",
    "        a, b, max_index = x_train, y_train, int(length_read * 0.9) - block_size - 1\n",
    "    else:  # split == 'test'\n",
    "        a, b, max_index = x_test, y_test, length_read - (int(length_read * 0.9) + block_size + 1)\n",
    "    \n",
    "    # Generate random indices for batch selection, ensuring they're within bounds\n",
    "    ix = torch.randint(0, max_index, (batch_size,))\n",
    "\n",
    "    # Initialize lists to hold the batches\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    for i in ix:\n",
    "        try:\n",
    "            # Extract the sequence from 'a' and the corresponding target from 'b'\n",
    "            seq = torch.tensor(a.iloc[i.item():i.item() + block_size].astype(np.float32).values, dtype=torch.float32)\n",
    "            target = torch.tensor(b.iloc[i.item() + block_size].astype(np.float32).values, dtype=torch.float32)\n",
    "            x_batch.append(seq)\n",
    "            y_batch.append(target)\n",
    "        except IndexError as e:\n",
    "            print(f\"IndexError for index {i.item()}: {str(e)}\")\n",
    "            print(f\"Attempting to access index [{i.item()}:{i.item() + block_size}] in 'a' with shape {a.shape}\")\n",
    "            print(f\"Attempting to access index {i.item() + block_size} in 'b' with shape {b.shape}\")\n",
    "            # Optionally, break or continue depending on desired behavior on error\n",
    "            break  # or continue\n",
    "\n",
    "    if not x_batch or not y_batch:\n",
    "        print(\"Error: Batch could not be created due to index issues.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Stack the collected sequences and targets into tensors\n",
    "    xstack = torch.stack(x_batch)\n",
    "    ystack = torch.stack(y_batch)\n",
    "\n",
    "    return xstack, ystack\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.keys = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.queries = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.values = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        K = self.keys(x)\n",
    "        Q = self.queries(x)\n",
    "        V = self.values(x)\n",
    "        \n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.embed_size ** 0.5\n",
    "        attention = torch.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        attended = torch.matmul(attention, V)\n",
    "        return attended\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        assert embed_size % num_heads == 0\n",
    "        \n",
    "        self.head_dim = embed_size // num_heads\n",
    "        \n",
    "        self.keys = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.queries = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.values = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        \n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _ = x.shape\n",
    "        \n",
    "        keys = self.keys(x).view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        queries = self.queries(x).view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        values = self.values(x).view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        \n",
    "        attention_scores = torch.einsum(\"bnqh,bnkh->bnqk\", [queries, keys]) / (self.head_dim ** 0.5)\n",
    "        attention = torch.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        attended = torch.einsum(\"bnqk,bnkv->bnqv\", [attention, values]).reshape(batch_size, seq_length, self.embed_size)\n",
    "        \n",
    "        output = self.fc_out(attended)\n",
    "        return output\n",
    "        \n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch3(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "    \n",
    "class EncoderWithFullyConnected(nn.Module):\n",
    "    def __init__(self, input_feature_dim, embed_size, seq_length, num_heads):\n",
    "        super(EncoderWithFullyConnected, self).__init__()\n",
    "        self.input_fc = nn.Linear(input_feature_dim, embed_size)\n",
    "        self.multi_head_attention = MultiHeadAttention(embed_size, num_heads)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, seq_length, embed_size))\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.input_fc(x)) + self.positional_encoding\n",
    "        x = self.multi_head_attention(x)\n",
    "        return x\n",
    "\n",
    "class EncoderDecoderModelWithMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_feature_dim, embed_size, target_dim, seq_length, num_heads):\n",
    "        super(EncoderDecoderModelWithMultiHeadAttention, self).__init__()\n",
    "        self.encoder = EncoderWithFullyConnected(input_feature_dim, embed_size, seq_length, num_heads)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embed_size, target_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x,targets):\n",
    "        encoded = self.encoder(x)\n",
    "        encoded_pooled = torch.mean(encoded, dim=1)\n",
    "        decoded = self.decoder(encoded_pooled)\n",
    "\n",
    "        loss = criterion(decoded, targets)\n",
    "\n",
    "        return decoded,loss\n",
    "        \n",
    "\n",
    "# Example usage\n",
    "'''\n",
    "if __name__ == \"__main__\":\n",
    "    input_feature_dim = 3  # Each input element is a 1x3 vector\n",
    "    embed_size = 32\n",
    "    target_dim = 3\n",
    "    seq_length = 6\n",
    "    num_heads = 8\n",
    "\n",
    "    model = EncoderDecoderModelWithMultiHeadAttention(input_feature_dim, embed_size, target_dim, seq_length, num_heads)\n",
    "    \n",
    "    # Example input (batch_size, seq_length, input_feature_dim)\n",
    "    x = torch.rand(4, seq_length, input_feature_dim)  # Batch size of 4\n",
    "    targets = torch.rand(4, target_dim)  # Corresponding targets\n",
    "\n",
    "    # Forward pass\n",
    "    predictions = model(x)\n",
    "    \n",
    "    # Example training components\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(predictions, targets)\n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"Loss:\", loss.item())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e24faa08-6f4e-45f4-bbaa-4f8bc6a8b1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 1.8066, val loss 1.8866\n",
      "step 100: train loss 0.0430, val loss 0.5770\n",
      "step 200: train loss 0.0025, val loss 0.0094\n",
      "step 300: train loss 0.0011, val loss 0.0129\n",
      "step 400: train loss 0.0009, val loss 0.0120\n",
      "step 500: train loss 0.0011, val loss 0.0146\n",
      "step 600: train loss 0.0009, val loss 0.0142\n",
      "step 700: train loss 0.0008, val loss 0.0124\n",
      "step 800: train loss 0.0011, val loss 0.0086\n",
      "step 900: train loss 0.0008, val loss 0.0119\n",
      "step 999: train loss 0.0008, val loss 0.0090\n",
      "Loss: 0.0003775508375838399\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = EncoderDecoderModelWithMultiHeadAttention(input_feature_dim, embed_size, target_dim, block_size, num_heads)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    for iter in range(max_iters):\n",
    "    \n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    \n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch3('train')\n",
    "        predictions,loss = model(xb,yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "477ee405-a643-47f9-9017-2354298d2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 200\n",
    "block_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21784269-77f7-43a7-ac86-fddbba3c0e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: (tensor([[ 1.6832,  1.5022, -0.9166]]), tensor(0.0135))\n",
      "actual: tensor([[ 1.8486,  1.3895, -0.8962]])\n"
     ]
    }
   ],
   "source": [
    "X_last_20 = x_test[-block_size-1:-1]\n",
    "y_last_20_actual = y_test[-1:]\n",
    "X_last_20_tensor = torch.tensor(X_last_20.astype(np.float32).values, dtype=torch.float32)\n",
    "y_last_20_actual_tensor = torch.tensor(y_last_20_actual.astype(np.float32).values, dtype=torch.float32)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Do not compute gradients\n",
    "    y_last_20_pred = model(X_last_20_tensor,y_last_20_actual_tensor)\n",
    "print(\"Predictions:\", y_last_20_pred)\n",
    "print (\"actual:\",y_last_20_actual_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cfcab00c-4550-41b2-b19f-121bfb613f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1254600, 3)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992b66fc-fdc0-48d0-9d34-5b7af6ca7184",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "for i in range(len(x_test) - 1001):\n",
    "    if i + 1000 < len(y_test):\n",
    "        y_window = y_test.iloc[i+1000]  # Use .iloc for safe access\n",
    "        # Assuming x_test and y_test are pandas DataFrames or Series and you need their values as tensors\n",
    "        seq = torch.tensor(x_test.iloc[i:i + block_size].astype(np.float32).values, dtype=torch.float32)\n",
    "        target = torch.tensor(y_test.iloc[i + block_size].astype(np.float32).values, dtype=torch.float32)\n",
    "    else:\n",
    "        # Handle the case where i+100 exceeds the bounds of y_test, if necessary\n",
    "        break\n",
    "    with torch.no_grad():  # Do not compute gradients\n",
    "        prediction,loss = model(x_window_tensor,y_window_tensor)  # Predict the next value based on the window\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# The actual values that correspond to the predictions\n",
    "# Since we predict the next value after each window, we start collecting actuals from the 101st value\n",
    "actuals = y_test[1000:].numpy()\n",
    "\n",
    "# Calculate Pearson's correlation coefficientc\n",
    "correlation_coef, p_value = pearsonr(predictions, actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626034b4-5f70-4270-b036-c7a2bcc4889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "print(x_train[:10])\n",
    "#Here we want to describe each line into its own token. \n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4 #how many sequences we use in parallel\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    # Select the correct data split\n",
    "    a, b, length_read2 = (x_train, y_train, int(length_read * 0.9)) if split == 'train' else (x_test, y_test, int(length_read * 0.1))\n",
    "    # Random indices for batch selection\n",
    "    ix = torch.randint(length_read2 - block_size, (batch_size,))\n",
    "\n",
    "    # Generate batches\n",
    "    xstack = torch.stack([torch.tensor(a.iloc[i.item():i.item() + block_size].astype(np.float64).values, dtype=torch.double) for i in ix])\n",
    "    # Assuming the target is structured similarly to the input and requires similar reshaping\n",
    "    ystack = torch.stack([torch.tensor(b.iloc[i.item()+block_size+1].astype(np.float64).values, dtype=torch.double) for i in ix])\n",
    "\n",
    "    return xstack, ystack\n",
    "xb,yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print('-------')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    context = xb[b,]\n",
    "    target = yb[b,]\n",
    "    print(f\"When input is {context.tolist()} the target: {target}\\n\")blocksize = 1000\n",
    "\n",
    "for t in range(blocksize):\n",
    "    context = x_train.iloc[:t+1].astype(np.float64)  # Selects rows up to index t from x\n",
    "    target = y_train.iloc[t].astype(np.float64)  # Selects the row at index t from y\n",
    "\n",
    "    # Flatten the context and target\n",
    "    flattened_context = torch.tensor(context.values,dtype=torch.float64)\n",
    "    flattened_target = torch.tensor(target.values,dtype=torch.float64)  # Assuming 'target' is a DataFrame. If 'target' is a Series, it's already 1-D.\n",
    "    if t < 10:\n",
    "    # Print the flattened arrays\n",
    "        print(f\"when the input is \\n{flattened_context} \\nthe target:\\n {flattened_target}\")\n",
    "\n",
    "def encode(x,y):\n",
    "    context = x.iloc[:t+1].astype(np.float64)  # Selects rows up to index t from x\n",
    "    target = y.iloc[t].astype(np.float64)  # Selects the row at index t from y\n",
    "\n",
    "    # Flatten the context and target\n",
    "    flattened_context = torch.tensor(context.values.flatten(),dtype=torch.float64)\n",
    "    flattened_target = torch.tensor(target.values.flatten(),dtype=torch.float64)  # Assuming 'target' is a DataFrame. If 'target' is a Series, it's already 1-D.\n",
    "    return flattened_context,flattened_target\n",
    "\n",
    "blocksize = 1000\n",
    "\n",
    "for t in range(blocksize):\n",
    "    context = x_train.iloc[:t+1].astype(np.float64)  # Selects rows up to index t from x\n",
    "    target = y_train.iloc[t].astype(np.float64)  # Selects the row at index t from y\n",
    "\n",
    "    # Flatten the context and target\n",
    "    flattened_context = torch.tensor(context.values,dtype=torch.float64)\n",
    "    flattened_target = torch.tensor(target.values,dtype=torch.float64)  # Assuming 'target' is a DataFrame. If 'target' is a Series, it's already 1-D.\n",
    "    if t < 10:\n",
    "    # Print the flattened arrays\n",
    "        print(f\"when the input is \\n{flattened_context} \\nthe target:\\n {flattened_target}\")\n",
    "\n",
    "def encode(x,y):\n",
    "    context = x.iloc[:t+1].astype(np.float64)  # Selects rows up to index t from x\n",
    "    target = y.iloc[t].astype(np.float64)  # Selects the row at index t from y\n",
    "\n",
    "    # Flatten the context and target\n",
    "    flattened_context = torch.tensor(context.values.flatten(),dtype=torch.float64)\n",
    "    flattened_target = torch.tensor(target.values.flatten(),dtype=torch.float64)  # Assuming 'target' is a DataFrame. If 'target' is a Series, it's already 1-D.\n",
    "    return flattened_context,flattened_target\n",
    "    \n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4 #how many sequences we use in parallel\n",
    "block_size = 8\n",
    "def get_batch(split):\n",
    "    #generate a small batch of data of inputs x and targets y\n",
    "    a, b, length_read2 = (x_train, y_train,length_read*0.9) if split == 'train' else (x_test, y_test,length_read*0.1)\n",
    "    ix = torch.randint(int(length_read2)-block_size, (batch_size, ))\n",
    "\n",
    "    # Flatten the context and target\n",
    "    xstack = torch.stack([torch.tensor(a.iloc[i.item():i.item()+block_size].astype(np.float64).values.flatten(), dtype=torch.double) for i in ix])\n",
    "    ystack = torch.stack([torch.tensor(b.iloc[i.item()+block_size+1].astype(np.float64).values.flatten(), dtype=torch.double) for i in ix])\n",
    "\n",
    "    return xstack,ystack\n",
    "xb,yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print('-------')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    context = xb[b,]\n",
    "    target = yb[b,]\n",
    "    print(f\"When input is {context.tolist()} the target: {target}\\n\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8544ab80-0407-436e-be08-c365dab619dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
