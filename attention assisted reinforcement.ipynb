{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e29198d0-f447-4c68-88b6-12d3f6ac6b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    The goal of this experiment will be to instanciate a reinforcement learning approach RNN on a quantum-setup. We will use continuous generated real data\\n    as interactions with the agent, and train the agent accordingly to the env. \\n    Think of this as like having a car learning to traverse a road, first, we give it a set environment, and the car runs on it will access to \\n    3 controls, if it is able to run, we can give it rewards, and if not we punish it and let it screw around with its controls.\\n    Inspiration: https://www.youtube.com/watch?v=cUojVsCJ51I\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    The goal of this experiment will be to instanciate a reinforcement learning approach RNN on a quantum-setup. We will use continuous generated real data\n",
    "    as interactions with the agent, and train the agent accordingly to the env. \n",
    "    Think of this as like having a car learning to traverse a road, first, we give it a set environment, and the car runs on it will access to \n",
    "    3 controls, if it is able to run, we can give it rewards, and if not we punish it and let it screw around with its controls.\n",
    "    Inspiration: https://www.youtube.com/watch?v=cUojVsCJ51I\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efe4eeea-b66a-494b-bb47-195b420bfd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import random\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque, namedtuple\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b56a46ab-3a0c-434d-81f3-abbed3c6d732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "def start_time():\n",
    "    return time.time()\n",
    "\n",
    "def elapsed(a):\n",
    "    return time.time()-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3471860f-df08-49a6-b173-9a65502eaaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_dim = 3  # Each input element is a 1x3 vector\n",
    "embed_size = 128\n",
    "target_dim = 3\n",
    "block_size = 100\n",
    "num_heads = 32\n",
    "max_iters = 1200\n",
    "batch_size = 32\n",
    "eval_iters = 200\n",
    "eval_interval = 10\n",
    "num_layers=12\n",
    "\n",
    "def get_batch3(split):\n",
    "    # Select the correct data split\n",
    "    if split == 'train':\n",
    "        a, b, max_index = x_train, y_train, int(length_read * 0.9) - block_size - 1\n",
    "    else:  # split == 'test'\n",
    "        a, b, max_index = x_test, y_test, length_read - (int(length_read * 0.9) + block_size + 1)\n",
    "\n",
    "    # Generate random indices for batch selection, ensuring they're within bounds\n",
    "    ix = torch.randint(0, max_index, (batch_size,))\n",
    "    # Initialize lists to hold the batches\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    for i in ix:\n",
    "        try:\n",
    "            # Extract sequences from 'a' and 'b' and the corresponding target from 'b'\n",
    "            seq_A = torch.tensor(a.iloc[i.item():i.item() + block_size+1].astype(np.float32).values, dtype=torch.float32)\n",
    "            seq_B = torch.tensor(b.iloc[i.item():i.item() + block_size].astype(np.float32).values, dtype=torch.float32)\n",
    "            target = torch.tensor(b.iloc[i.item() + block_size].astype(np.float32).values, dtype=torch.float32)\n",
    "\n",
    "            seq = torch.cat((seq_A, seq_B), dim=0)\n",
    "            x_batch.append(seq)\n",
    "            y_batch.append(target)\n",
    "        except IndexError as e:\n",
    "            print(f\"IndexError for index {i.item()}: {str(e)}\")\n",
    "            print(f\"Attempting to access index [{i.item()}:{i.item() + block_size}] in 'a' with shape {a.shape}\")\n",
    "            print(f\"Attempting to access index {i.item() + block_size} in 'b' with shape {b.shape}\")\n",
    "            # Optionally, break or continue depending on desired behavior on error\n",
    "            break  # or continue\n",
    "\n",
    "    if not x_batch or not y_batch:\n",
    "        print(\"Error: Batch could not be created due to index issues.\")\n",
    "        return None, None\n",
    "\n",
    "    # Stack the collected sequences and targets into tensors\n",
    "    xstack = torch.stack(x_batch)\n",
    "    ystack = torch.stack(y_batch)\n",
    "\n",
    "    return xstack, ystack\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.keys = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.queries = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.values = nn.Linear(embed_size, embed_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        K = self.keys(x)\n",
    "        Q = self.queries(x)\n",
    "        V = self.values(x)\n",
    "\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.embed_size ** 0.5\n",
    "        attention = torch.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        attended = torch.matmul(attention, V)\n",
    "        return attended\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert embed_size % num_heads == 0\n",
    "\n",
    "        self.head_dim = embed_size // num_heads\n",
    "\n",
    "        self.keys = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.queries = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.values = nn.Linear(embed_size, embed_size, bias=False)\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _ = x.shape\n",
    "        keys = self.keys(x).view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        queries = self.queries(x).view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        values = self.values(x).view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "\n",
    "        attention_scores = torch.einsum(\"bnqh,bnkh->bnqk\", [queries, keys]) / (self.head_dim ** 0.5)\n",
    "        attention = torch.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        attended = torch.einsum(\"bnqk,bnkv->bnqv\", [attention, values]).reshape(batch_size, seq_length, self.embed_size)\n",
    "\n",
    "        output = self.fc_out(attended)\n",
    "        return output\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch3(split)\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train\n",
    "    del X, Y\n",
    "    return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.attention = MultiHeadAttention(embed_size, num_heads)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, 2 * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * embed_size, embed_size),\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, value):\n",
    "        x = self.norm1(value)\n",
    "        attention_output = self.attention(x)\n",
    "        x = value + self.dropout1(attention_output)  # Residual connection and dropout after attention\n",
    "        x = self.norm2(x)\n",
    "        feed_forward_output = self.feed_forward(x)\n",
    "        out = value + self.dropout2(feed_forward_output)  # Residual connection and dropout after FFN\n",
    "        return out\n",
    "\n",
    "# Positional Encoding in Encoder class should be moved to the device\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_feature_dim, embed_size, num_heads, num_layers, seq_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_fc = nn.Linear(input_feature_dim, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, seq_length, embed_size)).to(device)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(embed_size, num_heads) for _ in range(num_layers)])\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.input_fc(x)) + self.positional_encoding\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def to_cpu(self):\n",
    "        # Move the entire model to CPU\n",
    "        self.input_fc.to('cpu')\n",
    "        self.positional_encoding.data = self.positional_encoding.data.cpu()\n",
    "        for layer in self.layers:\n",
    "            layer.to('cpu')\n",
    "        self.relu.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "class EncoderDecoderModelWithMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_feature_dim, embed_size, target_dim, seq_length, num_heads, num_layers):\n",
    "        super(EncoderDecoderModelWithMultiHeadAttention, self).__init__()\n",
    "        self.encoder = Encoder(input_feature_dim, embed_size, num_heads, num_layers, seq_length)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embed_size, target_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        encoded = self.encoder(x)\n",
    "        encoded_pooled = torch.mean(encoded, dim=1)\n",
    "        decoded = self.decoder(encoded_pooled)\n",
    "        \n",
    "        if targets is not None:\n",
    "            loss = criterion(decoded, targets)  \n",
    "            return decoded, loss\n",
    "\n",
    "\n",
    "        return decoded, None\n",
    "\n",
    "    def to_cpu(self):\n",
    "        self.encoder.to_cpu()\n",
    "        for layer in self.decoder:\n",
    "            layer.to('cpu')\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78e56f0d-b638-4928-b575-f2cd3d994625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from C:/Users/yueze/Desktop/trained_model.pth\n"
     ]
    }
   ],
   "source": [
    "model_path = \"C:/Users/yueze/Desktop/trained_model.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EncoderDecoderModelWithMultiHeadAttention(input_feature_dim, embed_size, target_dim, block_size+1, num_heads, num_layers)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "print(\"Model loaded from\", model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad8fb7f-e6e2-4e7b-8d15-ee9d52720f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f84221-370a-4b67-9145-f00e7078d374",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: [[-1.09914520e-01 -1.59248028e+00  1.01984249e+00]\n",
      " [-3.89330018e-01 -1.87189578e+00  7.40426997e-01]\n",
      " [ 2.27884673e-02 -1.45977730e+00  1.15254548e+00]\n",
      " [-5.13784451e-01 -1.99635022e+00  6.15972564e-01]\n",
      " [ 1.36503389e-01 -1.34606237e+00  1.26626040e+00]\n",
      " [-6.14483857e-01 -2.09704962e+00  5.15273157e-01]\n",
      " [ 2.22171781e-01 -1.26039398e+00  1.35192880e+00]\n",
      " [-6.83406581e-01 -2.16597235e+00  4.46350434e-01]\n",
      " [ 2.72969348e-01 -1.20959642e+00  1.40272636e+00]\n",
      " [-7.15062277e-01 -2.19762804e+00  4.14694738e-01]\n",
      " [ 2.84849584e-01 -1.19771618e+00  1.41460660e+00]\n",
      " [-7.06929270e-01 -2.18949503e+00  4.22827745e-01]\n",
      " [ 2.56866116e-01 -1.22569965e+00  1.38662313e+00]\n",
      " [-6.59655431e-01 -2.14222120e+00  4.70101583e-01]\n",
      " [ 1.91248093e-01 -1.29131767e+00  1.32100511e+00]\n",
      " [-5.77006568e-01 -2.05957233e+00  5.52750446e-01]\n",
      " [ 9.32226076e-02 -1.38934316e+00  1.22297962e+00]\n",
      " [-4.65566441e-01 -1.94813221e+00  6.64190573e-01]\n",
      " [-2.94016860e-02 -1.51196745e+00  1.10035533e+00]\n",
      " [-3.34212307e-01 -1.81677807e+00  7.95544707e-01]\n",
      " [-1.66856607e-01 -1.64942237e+00  9.62900408e-01]\n",
      " [-1.93407761e-01 -1.67597353e+00  9.36349254e-01]\n",
      " [-3.08192575e-01 -1.79075834e+00  8.21564440e-01]\n",
      " [-5.43692119e-02 -1.53693498e+00  1.07538780e+00]\n",
      " [-4.42150847e-01 -1.92471661e+00  6.87606167e-01]\n",
      " [ 7.18276087e-02 -1.41073816e+00  1.20158462e+00]\n",
      " [-5.58060386e-01 -2.04062615e+00  5.71696629e-01]\n",
      " [ 1.75129935e-01 -1.30743583e+00  1.30488695e+00]\n",
      " [-6.46687902e-01 -2.12925367e+00  4.83069112e-01]\n",
      " [ 2.47308761e-01 -1.23525700e+00  1.37706578e+00]\n",
      " [-7.00973380e-01 -2.18353914e+00  4.28783635e-01]\n",
      " [ 2.82614366e-01 -1.19995140e+00  1.41237138e+00]\n",
      " [-7.16592469e-01 -2.19915823e+00  4.13164546e-01]\n",
      " [ 2.78234323e-01 -1.20433144e+00  1.40799134e+00]\n",
      " [-6.92300961e-01 -2.17486673e+00  4.37456054e-01]\n",
      " [ 2.34517545e-01 -1.24804822e+00  1.36427456e+00]\n",
      " [-6.30033904e-01 -2.11259967e+00  4.99723111e-01]\n",
      " [ 1.54946485e-01 -1.32761928e+00  1.28470350e+00]\n",
      " [-5.34751456e-01 -2.01731722e+00  5.95005558e-01]\n",
      " [ 4.58597280e-02 -1.43670604e+00  1.17561674e+00]\n",
      " [-4.14043763e-01 -1.89660953e+00  7.15713252e-01]\n",
      " [-8.40529370e-02 -1.56661870e+00  1.04570408e+00]\n",
      " [-2.77526329e-01 -1.76009209e+00  8.52230686e-01]\n",
      " [-2.24442742e-01 -1.70700851e+00  9.05314273e-01]\n",
      " [-1.36074056e-01 -1.61863982e+00  9.93682959e-01]\n",
      " [-3.64126315e-01 -1.84669208e+00  7.65630699e-01]\n",
      " [-9.54949889e-04 -1.48352071e+00  1.12880206e+00]\n",
      " [-4.91976544e-01 -1.97454231e+00  6.37780471e-01]\n",
      " [ 1.17067478e-01 -1.36549829e+00  1.24682449e+00]\n",
      " [-5.97808952e-01 -2.08037472e+00  5.31948063e-01]\n",
      " [ 2.08591629e-01 -1.27397414e+00  1.33834864e+00]\n",
      " [-6.73192990e-01 -2.15575875e+00  4.56564025e-01]\n",
      " [ 2.66326742e-01 -1.21623902e+00  1.39608376e+00]\n",
      " [-7.12123609e-01 -2.19468937e+00  4.17633406e-01]\n",
      " [ 2.85673671e-01 -1.19689209e+00  1.41543069e+00]\n",
      " [-7.11499618e-01 -2.19406538e+00  4.18257397e-01]\n",
      " [ 2.65091250e-01 -1.21747451e+00  1.39484826e+00]\n",
      " [-6.71370724e-01 -2.15393649e+00  4.58386290e-01]\n",
      " [ 2.06219063e-01 -1.27634670e+00  1.33597608e+00]\n",
      " [-5.94933572e-01 -2.07749934e+00  5.34823442e-01]\n",
      " [ 1.13746836e-01 -1.36881893e+00  1.24350385e+00]\n",
      " [-4.88277102e-01 -1.97084287e+00  6.41479913e-01]\n",
      " [-4.95914789e-03 -1.48752491e+00  1.12479787e+00]\n",
      " [-3.59897506e-01 -1.84246327e+00  7.69859509e-01]\n",
      " [-1.40442838e-01 -1.62300860e+00  9.89314177e-01]\n",
      " [-2.20021429e-01 -1.70258719e+00  9.09735585e-01]\n",
      " [-2.81911680e-01 -1.76447744e+00  8.47845335e-01]\n",
      " [-7.97913206e-02 -1.56235709e+00  1.04996569e+00]\n",
      " [-4.18096348e-01 -1.90066211e+00  7.11660666e-01]\n",
      " [ 4.96221702e-02 -1.43294359e+00  1.17937918e+00]\n",
      " [-5.38148450e-01 -2.02071421e+00  5.91608565e-01]\n",
      " [ 1.57910039e-01 -1.32465573e+00  1.28766705e+00]\n",
      " [-6.32504703e-01 -2.11507047e+00  4.97252312e-01]\n",
      " [ 2.36446135e-01 -1.24611963e+00  1.36620315e+00]\n",
      " [-6.93648743e-01 -2.17621451e+00  4.36108272e-01]\n",
      " [ 2.78974320e-01 -1.20359144e+00  1.40873133e+00]\n",
      " [-7.16709870e-01 -2.19927563e+00  4.13047145e-01]\n",
      " [ 2.82106821e-01 -1.20045894e+00  1.41186384e+00]\n",
      " [-6.99851048e-01 -2.18241681e+00  4.29905966e-01]\n",
      " [ 2.45594106e-01 -1.23697166e+00  1.37535112e+00]\n",
      " [-6.44415242e-01 -2.12698101e+00  4.85341772e-01]\n",
      " [ 1.72344757e-01 -1.31022101e+00  1.30210177e+00]\n",
      " [-5.54818436e-01 -2.03738420e+00  5.74938578e-01]\n",
      " [ 6.81937748e-02 -1.41437199e+00  1.19795079e+00]\n",
      " [-4.38197860e-01 -1.92076362e+00  6.91559155e-01]\n",
      " [-5.85622333e-02 -1.54112800e+00  1.07119478e+00]\n",
      " [-3.03843442e-01 -1.78640921e+00  8.25913572e-01]\n",
      " [-1.97825956e-01 -1.68039172e+00  9.31931059e-01]\n",
      " [-1.62457779e-01 -1.64502354e+00  9.67299236e-01]\n",
      " [-3.38503725e-01 -1.82106949e+00  7.91253289e-01]\n",
      " [-2.53035704e-02 -1.50786933e+00  1.10445344e+00]\n",
      " [-4.69389230e-01 -1.95195499e+00  6.60367784e-01]\n",
      " [ 9.66935572e-02 -1.38587221e+00  1.22645057e+00]\n",
      " [-5.80056207e-01 -2.06262197e+00  5.49700807e-01]\n",
      " [ 1.93815383e-01 -1.28875038e+00  1.32357240e+00]\n",
      " [-6.61688988e-01 -2.14425475e+00  4.68068027e-01]\n",
      " [ 2.58325238e-01 -1.22424053e+00  1.38808225e+00]\n",
      " [-7.07784752e-01 -2.19035052e+00  4.21972263e-01]\n",
      " [ 2.85084303e-01 -1.19748146e+00  1.41484132e+00]\n",
      " [-7.14671536e-01 -2.19723730e+00  4.15085478e-01]\n",
      " [ 1.34574699e+00 -3.24676216e-01  1.15926051e+00]\n",
      " [ 0.00000000e+00  1.00000000e+00  2.20000000e+00]]\n",
      "Next state: [[-3.89330018e-01 -1.87189578e+00  7.40426997e-01]\n",
      " [ 2.27884673e-02 -1.45977730e+00  1.15254548e+00]\n",
      " [-5.13784451e-01 -1.99635022e+00  6.15972564e-01]\n",
      " [ 1.36503389e-01 -1.34606237e+00  1.26626040e+00]\n",
      " [-6.14483857e-01 -2.09704962e+00  5.15273157e-01]\n",
      " [ 2.22171781e-01 -1.26039398e+00  1.35192880e+00]\n",
      " [-6.83406581e-01 -2.16597235e+00  4.46350434e-01]\n",
      " [ 2.72969348e-01 -1.20959642e+00  1.40272636e+00]\n",
      " [-7.15062277e-01 -2.19762804e+00  4.14694738e-01]\n",
      " [ 2.84849584e-01 -1.19771618e+00  1.41460660e+00]\n",
      " [-7.06929270e-01 -2.18949503e+00  4.22827745e-01]\n",
      " [ 2.56866116e-01 -1.22569965e+00  1.38662313e+00]\n",
      " [-6.59655431e-01 -2.14222120e+00  4.70101583e-01]\n",
      " [ 1.91248093e-01 -1.29131767e+00  1.32100511e+00]\n",
      " [-5.77006568e-01 -2.05957233e+00  5.52750446e-01]\n",
      " [ 9.32226076e-02 -1.38934316e+00  1.22297962e+00]\n",
      " [-4.65566441e-01 -1.94813221e+00  6.64190573e-01]\n",
      " [-2.94016860e-02 -1.51196745e+00  1.10035533e+00]\n",
      " [-3.34212307e-01 -1.81677807e+00  7.95544707e-01]\n",
      " [-1.66856607e-01 -1.64942237e+00  9.62900408e-01]\n",
      " [-1.93407761e-01 -1.67597353e+00  9.36349254e-01]\n",
      " [-3.08192575e-01 -1.79075834e+00  8.21564440e-01]\n",
      " [-5.43692119e-02 -1.53693498e+00  1.07538780e+00]\n",
      " [-4.42150847e-01 -1.92471661e+00  6.87606167e-01]\n",
      " [ 7.18276087e-02 -1.41073816e+00  1.20158462e+00]\n",
      " [-5.58060386e-01 -2.04062615e+00  5.71696629e-01]\n",
      " [ 1.75129935e-01 -1.30743583e+00  1.30488695e+00]\n",
      " [-6.46687902e-01 -2.12925367e+00  4.83069112e-01]\n",
      " [ 2.47308761e-01 -1.23525700e+00  1.37706578e+00]\n",
      " [-7.00973380e-01 -2.18353914e+00  4.28783635e-01]\n",
      " [ 2.82614366e-01 -1.19995140e+00  1.41237138e+00]\n",
      " [-7.16592469e-01 -2.19915823e+00  4.13164546e-01]\n",
      " [ 2.78234323e-01 -1.20433144e+00  1.40799134e+00]\n",
      " [-6.92300961e-01 -2.17486673e+00  4.37456054e-01]\n",
      " [ 2.34517545e-01 -1.24804822e+00  1.36427456e+00]\n",
      " [-6.30033904e-01 -2.11259967e+00  4.99723111e-01]\n",
      " [ 1.54946485e-01 -1.32761928e+00  1.28470350e+00]\n",
      " [-5.34751456e-01 -2.01731722e+00  5.95005558e-01]\n",
      " [ 4.58597280e-02 -1.43670604e+00  1.17561674e+00]\n",
      " [-4.14043763e-01 -1.89660953e+00  7.15713252e-01]\n",
      " [-8.40529370e-02 -1.56661870e+00  1.04570408e+00]\n",
      " [-2.77526329e-01 -1.76009209e+00  8.52230686e-01]\n",
      " [-2.24442742e-01 -1.70700851e+00  9.05314273e-01]\n",
      " [-1.36074056e-01 -1.61863982e+00  9.93682959e-01]\n",
      " [-3.64126315e-01 -1.84669208e+00  7.65630699e-01]\n",
      " [-9.54949889e-04 -1.48352071e+00  1.12880206e+00]\n",
      " [-4.91976544e-01 -1.97454231e+00  6.37780471e-01]\n",
      " [ 1.17067478e-01 -1.36549829e+00  1.24682449e+00]\n",
      " [-5.97808952e-01 -2.08037472e+00  5.31948063e-01]\n",
      " [ 2.08591629e-01 -1.27397414e+00  1.33834864e+00]\n",
      " [-6.73192990e-01 -2.15575875e+00  4.56564025e-01]\n",
      " [ 2.66326742e-01 -1.21623902e+00  1.39608376e+00]\n",
      " [-7.12123609e-01 -2.19468937e+00  4.17633406e-01]\n",
      " [ 2.85673671e-01 -1.19689209e+00  1.41543069e+00]\n",
      " [-7.11499618e-01 -2.19406538e+00  4.18257397e-01]\n",
      " [ 2.65091250e-01 -1.21747451e+00  1.39484826e+00]\n",
      " [-6.71370724e-01 -2.15393649e+00  4.58386290e-01]\n",
      " [ 2.06219063e-01 -1.27634670e+00  1.33597608e+00]\n",
      " [-5.94933572e-01 -2.07749934e+00  5.34823442e-01]\n",
      " [ 1.13746836e-01 -1.36881893e+00  1.24350385e+00]\n",
      " [-4.88277102e-01 -1.97084287e+00  6.41479913e-01]\n",
      " [-4.95914789e-03 -1.48752491e+00  1.12479787e+00]\n",
      " [-3.59897506e-01 -1.84246327e+00  7.69859509e-01]\n",
      " [-1.40442838e-01 -1.62300860e+00  9.89314177e-01]\n",
      " [-2.20021429e-01 -1.70258719e+00  9.09735585e-01]\n",
      " [-2.81911680e-01 -1.76447744e+00  8.47845335e-01]\n",
      " [-7.97913206e-02 -1.56235709e+00  1.04996569e+00]\n",
      " [-4.18096348e-01 -1.90066211e+00  7.11660666e-01]\n",
      " [ 4.96221702e-02 -1.43294359e+00  1.17937918e+00]\n",
      " [-5.38148450e-01 -2.02071421e+00  5.91608565e-01]\n",
      " [ 1.57910039e-01 -1.32465573e+00  1.28766705e+00]\n",
      " [-6.32504703e-01 -2.11507047e+00  4.97252312e-01]\n",
      " [ 2.36446135e-01 -1.24611963e+00  1.36620315e+00]\n",
      " [-6.93648743e-01 -2.17621451e+00  4.36108272e-01]\n",
      " [ 2.78974320e-01 -1.20359144e+00  1.40873133e+00]\n",
      " [-7.16709870e-01 -2.19927563e+00  4.13047145e-01]\n",
      " [ 2.82106821e-01 -1.20045894e+00  1.41186384e+00]\n",
      " [-6.99851048e-01 -2.18241681e+00  4.29905966e-01]\n",
      " [ 2.45594106e-01 -1.23697166e+00  1.37535112e+00]\n",
      " [-6.44415242e-01 -2.12698101e+00  4.85341772e-01]\n",
      " [ 1.72344757e-01 -1.31022101e+00  1.30210177e+00]\n",
      " [-5.54818436e-01 -2.03738420e+00  5.74938578e-01]\n",
      " [ 6.81937748e-02 -1.41437199e+00  1.19795079e+00]\n",
      " [-4.38197860e-01 -1.92076362e+00  6.91559155e-01]\n",
      " [-5.85622333e-02 -1.54112800e+00  1.07119478e+00]\n",
      " [-3.03843442e-01 -1.78640921e+00  8.25913572e-01]\n",
      " [-1.97825956e-01 -1.68039172e+00  9.31931059e-01]\n",
      " [-1.62457779e-01 -1.64502354e+00  9.67299236e-01]\n",
      " [-3.38503725e-01 -1.82106949e+00  7.91253289e-01]\n",
      " [-2.53035704e-02 -1.50786933e+00  1.10445344e+00]\n",
      " [-4.69389230e-01 -1.95195499e+00  6.60367784e-01]\n",
      " [ 9.66935572e-02 -1.38587221e+00  1.22645057e+00]\n",
      " [-5.80056207e-01 -2.06262197e+00  5.49700807e-01]\n",
      " [ 1.93815383e-01 -1.28875038e+00  1.32357240e+00]\n",
      " [-6.61688988e-01 -2.14425475e+00  4.68068027e-01]\n",
      " [ 2.58325238e-01 -1.22424053e+00  1.38808225e+00]\n",
      " [-7.07784752e-01 -2.19035052e+00  4.21972263e-01]\n",
      " [ 2.85084303e-01 -1.19748146e+00  1.41484132e+00]\n",
      " [-7.14671536e-01 -2.19723730e+00  4.15085478e-01]\n",
      " [-7.18031528e-01 -1.24597340e+00 -1.07794144e+00]\n",
      " [ 1.42715907e+00 -5.43667972e-01  1.10708690e+00]\n",
      " [ 0.00000000e+00  1.00000000e+00  2.20000000e+00]]\n",
      "Next state size: 306\n",
      "Reward: -6.326210721395564\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "class DummyHardware:\n",
    "    \"\"\"Simulates hardware behavior for the environment.\"\"\"\n",
    "    @staticmethod\n",
    "    def implement(actions,t, measurement_1350, measurement_1550):\n",
    "        # Apply action effects with cosine function\n",
    "        measurement_1350 += actions * 0.3 * np.cos(actions)+ np.sin(t)\n",
    "        # Wrap around if the value exceeds 2.2 or goes below -2.2\n",
    "        \n",
    "        while np.any(measurement_1350 > 2.2) or np.any(measurement_1350 < -2.2):\n",
    "            measurement_1350 = np.where(measurement_1350 > 2.2, measurement_1350 - 4.4, measurement_1350)\n",
    "            measurement_1350 = np.where(measurement_1350 < -2.2, measurement_1350 + 4.4, measurement_1350)\n",
    "        \n",
    "        # Apply action to 1550 nm state with a multiplier of 1\n",
    "        measurement_1550 += actions * np.cos(3 * actions) + np.sin(3 * t)\n",
    "        \n",
    "        while np.any(measurement_1550 > 2.2) or np.any(measurement_1550 < -2.2):\n",
    "            measurement_1550 = np.where(measurement_1550 > 2.2, measurement_1550 - 4.4, measurement_1550)\n",
    "            measurement_1550 = np.where(measurement_1550 < -2.2, measurement_1550 + 4.4, measurement_1550)    \n",
    "        return measurement_1350, measurement_1550    \n",
    "\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def drift(t, measurement_1350, measurement_1550):\n",
    "        # Apply action effects with cosine function\n",
    "        measurement_1350 += np.sin(t)\n",
    "        while np.any(measurement_1350 > 2.2) or np.any(measurement_1350 < -2.2):\n",
    "            measurement_1350 = np.where(measurement_1350 > 2.2, measurement_1350 - 4.4, measurement_1350)\n",
    "            measurement_1350 = np.where(measurement_1350 < -2.2, measurement_1350 + 4.4, measurement_1350)\n",
    "        measurement_1550 += np.sin(3 * t)\n",
    "        while np.any(measurement_1550 > 2.2) or np.any(measurement_1550 < -2.2):\n",
    "            measurement_1550 = np.where(measurement_1550 > 2.2, measurement_1550 - 4.4, measurement_1550)\n",
    "            measurement_1550 = np.where(measurement_1550 < -2.2, measurement_1550 + 4.4, measurement_1550)\n",
    "   \n",
    "        return measurement_1350, measurement_1550\n",
    "\n",
    "    @staticmethod\n",
    "    def measure(wavelength, t, initial_state_1550=None):\n",
    "        # Simulate a hardware measurement with random values within a range\n",
    "        if wavelength == 1350:\n",
    "            return env.optimal_state + np.sin(t)\n",
    "        elif wavelength == 1550 and initial_state_1550 is not None:\n",
    "            return initial_state_1550 + np.sin(3 * t)\n",
    "\n",
    "\n",
    "\n",
    "class Env:\n",
    "    \"\"\"Wrapper for interacting with the environment.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.optimal_state = np.array([0, 1, 2.2])\n",
    "        self.df_1550 = []\n",
    "        self.t = 0\n",
    "        #these two variables are for after the action executes\n",
    "        self.measured_1550 = None\n",
    "        self.measured_1350= None\n",
    "        #these two variables are for after the action executes, and we actually measure. (the dummy environment needs manually updating, in real case,\n",
    "        #these two variables should be the same.\n",
    "        self.drifted_1550= None\n",
    "        self.drifted_1350=None\n",
    "        \n",
    "    def reset(self):\n",
    "        self.t = 0  # Reset time\n",
    "        action = np.array([0,0,0])\n",
    "        # Reset the measurement at 1350 nm perfectly to the optimal state\n",
    "        self.measured_1350 = np.array(DummyHardware.measure(1350,self.t))\n",
    "        self.data_to_correct_for = self.optimal_state - self.measured_1350\n",
    "        action = self.data_to_correct_for  # Directly use the correction values as actions\n",
    "        \n",
    "        # Reset the measurement at 1550 nm to some random position\n",
    "        self.measured_1550 = np.random.uniform(-2, 2, 3)\n",
    "        \n",
    "        while len(self.df_1550) < 100:\n",
    "            measured_1350=copy.deepcopy(self.measured_1350)\n",
    "            measured_1550=copy.deepcopy(self.measured_1550)\n",
    "            self.t += 1  # Increment time\n",
    "            # Implement the action in the hardware with effects on states\n",
    "            self.measured_1350, self.measured_1550 = DummyHardware.implement(action,self.t ,measured_1350, measured_1550)  \n",
    "            # Insert new measurement into the list\n",
    "            self.df_1550.append(copy.deepcopy(self.measured_1550))\n",
    "        if len(self.df_1550) > 100:\n",
    "            self.df_1550.pop(0)  # Remove the oldest element\n",
    "        stacked_df = np.stack(self.df_1550)\n",
    "        \n",
    "        # Ensure optimal_state is reshaped correctly\n",
    "        optimal_state_reshaped = self.optimal_state.reshape(1, -1)\n",
    "        \n",
    "        # Concatenate optimal_state to the end\n",
    "        stacked_df_with_optimal = np.concatenate((stacked_df, optimal_state_reshaped), axis=0)\n",
    "        \n",
    "        # Convert the final array to a torch tensor\n",
    "        tensor_df = torch.tensor(stacked_df_with_optimal, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Get the output from the model\n",
    "        output, _ = model(tensor_df, None)\n",
    "        \n",
    "        # Next state is the latest 100 measurements of 1550 concatenated with the output\n",
    "        next_state_measurements = np.array(self.df_1550)\n",
    "        output_numpy = output.cpu().detach().numpy().reshape(1, -1)\n",
    "        \n",
    "        # Concatenate output and optimal state with the measurements\n",
    "        next_state = np.concatenate((next_state_measurements, output_numpy, optimal_state_reshaped), axis=0)\n",
    "        return next_state\n",
    "\n",
    "    def step(self, action):\n",
    "        measured_1350=copy.deepcopy(self.measured_1350)\n",
    "        measured_1550=copy.deepcopy(self.measured_1550)\n",
    "        self.t += 1  # Increment time\n",
    "            # Implement the action in the hardware with effects on states\n",
    "        self.measured_1350, self.measured_1550 = DummyHardware.implement(action,self.t ,measured_1350, measured_1550)\n",
    "            # Insert new measurement into the list\n",
    "        self.df_1550.append(copy.deepcopy(self.measured_1550))\n",
    "        if len(self.df_1550) > 100:\n",
    "            self.df_1550.pop(0)  # Remove the oldest element\n",
    "        stacked_df = np.stack(self.df_1550)\n",
    "        \n",
    "        # Ensure optimal_state is reshaped correctly\n",
    "        optimal_state_reshaped = self.optimal_state.reshape(1, -1)\n",
    "        \n",
    "        # Concatenate optimal_state to the end\n",
    "        stacked_df_with_optimal = np.concatenate((stacked_df, optimal_state_reshaped), axis=0)\n",
    "        \n",
    "        # Convert the final array to a torch tensor\n",
    "        tensor_df = torch.tensor(stacked_df_with_optimal, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Get the output from the model\n",
    "        output, _ = model(tensor_df, None)\n",
    "        \n",
    "        # Next state is the latest 100 measurements of 1550 concatenated with the output\n",
    "        next_state_measurements = np.array(self.df_1550)\n",
    "        output_numpy = output.cpu().detach().numpy().reshape(1, -1)\n",
    "        \n",
    "        # Concatenate output and optimal state with the measurements\n",
    "        next_state = np.concatenate((next_state_measurements, output_numpy, optimal_state_reshaped), axis=0)\n",
    "        \n",
    "        # Reward is the negative mean squared error between new_measurement_1350 and optimal_state\n",
    "        reward = -np.mean((self.measured_1350 - self.optimal_state) ** 2)\n",
    "        \n",
    "        return next_state, reward\n",
    "\n",
    "# Example usage\n",
    "env = Env()\n",
    "state = env.reset()\n",
    "print(f\"Initial state: {state}\")\n",
    "\n",
    "action = np.array([1, -0.5, 2])  # Example action\n",
    "# Simulate a step\n",
    "next_state, reward = env.step(action)\n",
    "print(f\"Next state: {next_state}\")\n",
    "print(f\"Next state size: {next_state.size}\")\n",
    "print(f\"Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec551d20-5139-449c-ba25-a9645c2c9072",
   "metadata": {},
   "outputs": [],
   "source": [
    "class env2():\n",
    "    \"\"\"\"This will be the wrapper for interacting with the environment\"\"\"\n",
    "    def reset():\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        pseudocode: \n",
    "            optimal_state = [a,b,c]\n",
    "            Take measurement result of 1350 (at the output of the fibre) [a',b',c']\n",
    "            Data to correct for = [a-a',b-b',c-c']\n",
    "            action = (sign(a-a'),abs(a-a'))x3\n",
    "            hardware.implement(Data to correct for )\n",
    "            Take measurement result of 1550 [d',e',f']\n",
    "            return ([d',e',f'])\n",
    "        \"\"\"\n",
    "        example_output_state = np.array([0,0,0])\n",
    "        return (example_output_state)\n",
    "        \n",
    "    def step(action):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        *action is a 6 valued vector containing 3 pairs of ((0,1), int) tuples\n",
    "        Since this step is also responsible for spawning the state, we wil also add in the \n",
    "        GPT- suggestion rates here.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        harware.implement (action)\n",
    "        sleep(1000)\n",
    "        new_1350 = hardware.measure (1350)\n",
    "        new_1550 - hardware.measure (1550)\n",
    "        df.insert(new_1350, new_1550)\n",
    "        next_state = df[-100:]\n",
    "        reward = = MSE(new_1350,optimal_state)\n",
    "        \n",
    "        \"\"\"\n",
    "        return (next_state,reward)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e7419e-a68f-49e3-a64a-9c201b084ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea6c40d4-b771-49dc-9f16-dd085ea12031",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size, layer_size, n_step, seed, layer_type=\"ff\"):\n",
    "        super(IQN, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.input_shape = state_size\n",
    "        self.action_size = action_size\n",
    "        self.K = 1  # Set K to 1 for single step\n",
    "        self.N = 1  # Set N to 1 for single step\n",
    "        self.n_cos = 64\n",
    "        self.layer_size = layer_size\n",
    "        self.pis = torch.FloatTensor([np.pi * i for i in range(self.n_cos)]).view(1, 1, self.n_cos).to(device)\n",
    "\n",
    "        self.head = nn.Linear(self.input_shape[0], layer_size)  # Could be a CNN\n",
    "        self.cos_embedding = nn.Linear(self.n_cos, layer_size)\n",
    "        self.ff_1 = nn.Linear(layer_size, layer_size)\n",
    "        self.ff_2 = nn.Linear(layer_size, action_size)\n",
    "\n",
    "    def calc_cos(self, batch_size, n_tau=1):\n",
    "        \"\"\"Calculate the cosine values for the number of tau samples.\"\"\"\n",
    "        taus = torch.rand(batch_size, n_tau).to(device).unsqueeze(-1)  # (batch_size, n_tau, 1)\n",
    "        cos = torch.cos(taus * self.pis)\n",
    "        assert cos.shape == (batch_size, n_tau, self.n_cos), \"cos shape is incorrect\"\n",
    "        return cos, taus\n",
    "\n",
    "    def forward(self, input, num_tau=1):\n",
    "        \"\"\"Quantile calculation depending on the number of tau.\"\"\"\n",
    "        print(f\"Input shape: {input.shape}\")\n",
    "        batch_size = input.shape[0]\n",
    "        print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "        x = torch.relu(self.head(input))\n",
    "        print(f\"After head and ReLU, x shape: {x.shape}\")\n",
    "\n",
    "        cos, taus = self.calc_cos(batch_size, num_tau)\n",
    "        print(f\"Cos shape: {cos.shape}, Taus shape: {taus.shape}\")\n",
    "\n",
    "        cos = cos.view(batch_size * num_tau, self.n_cos)\n",
    "        print(f\"Reshaped cos: {cos.shape}\")\n",
    "\n",
    "        cos_x = torch.relu(self.cos_embedding(cos)).view(batch_size, num_tau, self.layer_size)\n",
    "        print(f\"After cos embedding and ReLU, cos_x shape: {cos_x.shape}\")\n",
    "\n",
    "        # Adjust x to match cos_x shape\n",
    "        x = (x.unsqueeze(1) * cos_x).view(batch_size * num_tau, self.layer_size)\n",
    "        print(f\"After element-wise multiplication and reshaping, x shape: {x.shape}\")\n",
    "\n",
    "        x = torch.relu(self.ff_1(x))\n",
    "        print(f\"After first feed-forward layer and ReLU, x shape: {x.shape}\")\n",
    "\n",
    "        out = self.ff_2(x)\n",
    "        print(f\"After second feed-forward layer, out shape: {out.shape}\")\n",
    "\n",
    "        final_out = out.view(batch_size, num_tau, self.action_size)\n",
    "        print(f\"Final output shape: {final_out.shape}\")\n",
    "\n",
    "        return final_out, taus\n",
    "\n",
    "    def get_action(self, inputs):\n",
    "        quantiles, _ = self.forward(inputs, self.K)\n",
    "        actions = quantiles.mean(dim=1)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beaf798b-2e13-460d-b78a-fb2c9c0d2a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size, device, seed, gamma, n_step=1):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        self.gamma = gamma\n",
    "        self.n_step = n_step\n",
    "        self.n_step_buffer = deque(maxlen=self.n_step)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        #print(\"before:\", state,action,reward,next_state, done)\n",
    "        self.n_step_buffer.append((state, action, reward, next_state, done))\n",
    "        if len(self.n_step_buffer) == self.n_step:\n",
    "            state, action, reward, next_state, done = self.calc_multistep_return()\n",
    "            #print(\"after:\",state,action,reward,next_state, done)\n",
    "            e = self.experience(state, action, reward, next_state, done)\n",
    "            self.memory.append(e)\n",
    "    \n",
    "    def calc_multistep_return(self):\n",
    "        Return = 0\n",
    "        for idx in range(self.n_step):\n",
    "            Return += self.gamma**idx * self.n_step_buffer[idx][2]\n",
    "        \n",
    "        return self.n_step_buffer[0][0], self.n_step_buffer[0][1], Return, self.n_step_buffer[-1][3], self.n_step_buffer[-1][4]\n",
    "        \n",
    "    \n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.stack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.stack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "893f8be1-4cbc-4c59-9d51-127cb6bfbc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TODO:\n",
    "Here, the following code for the agent are directly copied from the paper,\n",
    "Since no testing has been made, modifications to be seen.\n",
    "\"\"\"      \n",
    "\n",
    "class DQN_Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 state_size,\n",
    "                 action_size,\n",
    "                 layer_size,\n",
    "                 n_step,\n",
    "                 BATCH_SIZE,\n",
    "                 BUFFER_SIZE,\n",
    "                 LR,\n",
    "                 TAU,\n",
    "                 GAMMA,\n",
    "                 UPDATE_EVERY,\n",
    "                 device,\n",
    "                 seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            layer_size (int): size of the hidden layer\n",
    "            BATCH_SIZE (int): size of the training batch\n",
    "            BUFFER_SIZE (int): size of the replay memory\n",
    "            LR (float): learning rate\n",
    "            TAU (float): tau for soft updating the network weights\n",
    "            GAMMA (float): discount factor\n",
    "            UPDATE_EVERY (int): update frequency\n",
    "            device (str): device that is used for the compute\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.device = device\n",
    "        self.TAU = TAU\n",
    "        self.GAMMA = GAMMA\n",
    "        self.UPDATE_EVERY = UPDATE_EVERY\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.Q_updates = 0\n",
    "        self.n_step = n_step\n",
    "\n",
    "        self.action_step = 4\n",
    "        self.last_action = None\n",
    "\n",
    "        # IQN-Network\n",
    "        self.qnetwork_local = IQN(state_size, action_size,layer_size, n_step, seed).to(device)\n",
    "        self.qnetwork_target = IQN(state_size, action_size,layer_size, n_step, seed).to(device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "        print(self.qnetwork_local)\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, self.device, seed, self.GAMMA, n_step)\n",
    "        \n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done, writer):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                loss = self.learn(experiences)\n",
    "                self.Q_updates += 1\n",
    "                writer.add_scalar(\"Q_loss\", loss, self.Q_updates)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy. Acting only every 4 frames!\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            frame: to adjust epsilon\n",
    "            state (array_like): current state\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        if self.action_step == 4:\n",
    "            state = np.array(state)\n",
    "\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "            self.qnetwork_local.eval()\n",
    "            with torch.no_grad():\n",
    "                action_values = self.qnetwork_local.get_action(state)\n",
    "            self.qnetwork_local.train()\n",
    "\n",
    "            # Epsilon-greedy action selection\n",
    "            if random.random() > eps: # select greedy action if random number is higher than epsilon or noisy network is used!\n",
    "                action = np.argmax(action_values.cpu().data.numpy())\n",
    "                self.last_action = action\n",
    "                return action\n",
    "            else:\n",
    "                action = random.choice(np.arange(self.action_size))\n",
    "                self.last_action = action \n",
    "                return action\n",
    "\n",
    "        else:\n",
    "            self.action_step += 1\n",
    "            return self.last_action\n",
    "\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next, _ = self.qnetwork_target(next_states)\n",
    "        Q_targets_next = Q_targets_next.detach().max(2)[0].unsqueeze(1) # (batch_size, 1, N)\n",
    "        \n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards.unsqueeze(-1) + (self.GAMMA**self.n_step * Q_targets_next * (1. - dones.unsqueeze(-1)))\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected, taus = self.qnetwork_local(states)\n",
    "        Q_expected = Q_expected.gather(2, actions.unsqueeze(-1).expand(self.BATCH_SIZE, 102, 1))\n",
    "\n",
    "        # Quantile Huber loss\n",
    "        td_error = Q_targets - Q_expected\n",
    "        assert td_error.shape == (self.BATCH_SIZE, 102, 3), \"wrong td error shape\"\n",
    "        huber_l = calculate_huber_loss(td_error, 1.0)\n",
    "        \n",
    "        quantil_l = abs(taus -(td_error.detach() < 0).float()) * huber_l / 1.0\n",
    "        \n",
    "        loss = quantil_l.sum(dim=1).mean(dim=1) # , keepdim=True if per weights get multipl\n",
    "        loss = loss.mean()\n",
    "\n",
    "\n",
    "        # Minimize the loss\n",
    "        loss.backward()\n",
    "        #clip_grad_norm_(self.qnetwork_local.parameters(),1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target)\n",
    "        return loss.detach().cpu().numpy()            \n",
    "\n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.TAU*local_param.data + (1.0-self.TAU)*target_param.data)\n",
    "\n",
    "\n",
    "def calculate_huber_loss(td_errors, k=1.0):\n",
    "    \"\"\"\n",
    "    Calculate huber loss element-wisely depending on kappa k.\n",
    "    \"\"\"\n",
    "    loss = torch.where(td_errors.abs() <= k, 0.5 * td_errors.pow(2), k * (td_errors.abs() - 0.5 * k))\n",
    "    assert loss.shape == (td_errors.shape[0],102, 3), \"huber loss has wrong shape\"\n",
    "    return loss\n",
    "    \n",
    "            \n",
    "def eval_runs(eps, frame):\n",
    "    \"\"\"\n",
    "    Makes an evaluation run with the current epsilon\n",
    "    \"\"\"\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    reward_batch = []\n",
    "    for i in range(5):\n",
    "        state = env.reset()\n",
    "        rewards = 0\n",
    "        while True:\n",
    "            action = agent.act(state, eps)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards += reward\n",
    "            if done:\n",
    "                break\n",
    "        reward_batch.append(rewards)\n",
    "        \n",
    "    writer.add_scalar(\"Reward\", np.mean(reward_batch), frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a468787f-602f-4f61-83b8-52f4948fe2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eps):\n",
    "    \"\"\"tells you how good is the current policy is performing\"\"\"\n",
    "    reward_batch = []\n",
    "    state = env_reset()\n",
    "    rewards=0\n",
    "    a = start_time()\n",
    "    while  (elapsed(a)<300):#todo :\n",
    "        action = agent.act(state,0.001,eval=True)\n",
    "        next_state, reward = env.step(action,optimal_state)\n",
    "        rewards+=reward\n",
    "    reward_batch.append(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18fc47e5-b1b3-4f51-ad7f-c387783ee0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(interaction_time = 10000,reset_time = 300,eps_fixed=False, end_exploration_time = 3e5, min_eps=0.01, eval_every=300, eval_runs=1, worker=1,optimal_state=[0,0,0] ): \n",
    "    \"\"\"Params\n",
    "    interaction_time = each agent in one epoch takes 300 (seconds) to complete\n",
    "    epsilon: the \"fuck around\" parameter; how much does the model weights fluctuate with each observation\n",
    "    end_exploration_time = how long til we stop annealing \n",
    "    min_eps = smallest learning rate we will have\n",
    "    eval_every = The frequency of evaluation runs. The agent's performance will be evaluated every eval_every seconds.\n",
    "    eval_runs = The number of evaluation runs to perform each time the agent is evaluated. This helps in assessing the agent's performance more robustly by averaging the results over multiple runs.\n",
    "    worker = The worker multiplier used for distributed training setups. Due to having only one computer setup, this parameter will always be 1.\n",
    "    optimal_state = the state we want to practice convergence on, we will use this as a reference for reward giving\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = []   \n",
    "    if eps_fixed:\n",
    "        eps = 0\n",
    "    else:\n",
    "        eps = 1\n",
    "    eps_start = 1\n",
    "    d_eps = eps_start - min_eps\n",
    "    i_episode = 1\n",
    "    state = env.reset()\n",
    "    score = 0      \n",
    "    done=False\n",
    "    for time in range (1, interaction_time+1):\n",
    "        if (time%reset_time==0 ):\n",
    "            done = True\n",
    "        action = agent.act(state,eps)\n",
    "        next_state, reward = env.step(action)\n",
    "        agent.step(state,action,reward,next_state,done,writer)\n",
    "        if eps_fixed == False:\n",
    "            eps = max(eps_start - ((time*d_eps)/end_exploration_time), min_eps)\n",
    "        \n",
    "        if done:\n",
    "            scores_window.append(score)  # save most recent score\n",
    "            scores.append(score)  # save most recent score\n",
    "            writer.add_scalar(\"Average100\", np.mean(scores_window), frame)\n",
    "            output_history.append(np.mean(scores_window))\n",
    "            print('\\rEpisode {}\\tFrame {}\\tAverage Score: {:.2f}'.format(i_episode, frame, np.mean(scores_window)), end=\"\")\n",
    "            if i_episode % 100 == 0:\n",
    "                print('\\rEpisode {}\\tFrame {}\\tAverage Score: {:.2f}'.format(i_episode, frame, np.mean(scores_window)))\n",
    "            i_episode += 1\n",
    "            state = env.reset()\n",
    "            score = 0\n",
    "            done = False\n",
    "\n",
    "    return output_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d63a268-9208-4efd-98b3-d91088788d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: create a dummy hardware-interaction layer that just returns gibberish and see how \n",
    "#we can added/interface with the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ff6b496-f85d-421e-ab00-50b376b7fd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using  cuda:0\n",
      "IQN(\n",
      "  (head): Linear(in_features=3, out_features=256, bias=True)\n",
      "  (cos_embedding): Linear(in_features=64, out_features=256, bias=True)\n",
      "  (ff_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (ff_2): Linear(in_features=256, out_features=3, bias=True)\n",
      ")\n",
      "Input shape: torch.Size([1, 102, 3])\n",
      "Batch size: 1\n",
      "After head and ReLU, x shape: torch.Size([1, 102, 256])\n",
      "Cos shape: torch.Size([1, 1, 64]), Taus shape: torch.Size([1, 1, 1])\n",
      "Reshaped cos: torch.Size([1, 64])\n",
      "After cos embedding and ReLU, cos_x shape: torch.Size([1, 1, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 256]' is invalid for input of size 26112",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m eps_fixed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     33\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 34\u001b[0m final_average100 \u001b[38;5;241m=\u001b[39m run(interaction_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m,reset_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m,eps_fixed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, end_exploration_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3e5\u001b[39m, min_eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, eval_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, eval_runs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, worker\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,optimal_state\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     35\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[1;32mIn[11], line 27\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(interaction_time, reset_time, eps_fixed, end_exploration_time, min_eps, eval_every, eval_runs, worker, optimal_state)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (time\u001b[38;5;241m%\u001b[39mreset_time\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m ):\n\u001b[0;32m     26\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state,eps)\n\u001b[0;32m     28\u001b[0m next_state, reward \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     29\u001b[0m agent\u001b[38;5;241m.\u001b[39mstep(state,action,reward,next_state,done,writer)\n",
      "Cell \u001b[1;32mIn[9], line 95\u001b[0m, in \u001b[0;36mDQN_Agent.act\u001b[1;34m(self, state, eps)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnetwork_local\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 95\u001b[0m     action_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnetwork_local\u001b[38;5;241m.\u001b[39mget_action(state)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnetwork_local\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Epsilon-greedy action selection\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 59\u001b[0m, in \u001b[0;36mIQN.get_action\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m---> 59\u001b[0m     quantiles, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK)\n\u001b[0;32m     60\u001b[0m     actions \u001b[38;5;241m=\u001b[39m quantiles\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m actions\n",
      "Cell \u001b[1;32mIn[16], line 44\u001b[0m, in \u001b[0;36mIQN.forward\u001b[1;34m(self, input, num_tau)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter cos embedding and ReLU, cos_x shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcos_x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Adjust x to match cos_x shape\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m x \u001b[38;5;241m=\u001b[39m (x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m cos_x)\u001b[38;5;241m.\u001b[39mview(batch_size \u001b[38;5;241m*\u001b[39m num_tau, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_size)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter element-wise multiplication and reshaping, x shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff_1(x))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[1, 256]' is invalid for input of size 26112"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    writer = SummaryWriter(\"runs/\"+\"IQN_CP_5\")\n",
    "    action_size = 3\n",
    "    state_size = (3,102)\n",
    "    layer_size=256\n",
    "    seed = 5\n",
    "    batch=8\n",
    "    BUFFER_SIZE = 100000\n",
    "    BATCH_SIZE=1\n",
    "    GAMMA = 0.99\n",
    "    TAU = 1e-2\n",
    "    LR = 1e-3\n",
    "    UPDATE_EVERY = 1\n",
    "    n_step = 1\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using \", device)\n",
    "\n",
    "    agent = DQN_Agent(state_size=state_size,    \n",
    "                        action_size=action_size,\n",
    "                        layer_size=256,\n",
    "                        n_step=n_step,\n",
    "                        BATCH_SIZE=BATCH_SIZE, \n",
    "                        BUFFER_SIZE=BUFFER_SIZE, \n",
    "                        LR=LR, \n",
    "                        TAU=TAU, \n",
    "                        GAMMA=GAMMA, \n",
    "                        UPDATE_EVERY=UPDATE_EVERY, \n",
    "                        device=device, \n",
    "                        seed=seed)\n",
    "\n",
    "    eps_fixed = False\n",
    "\n",
    "    t0 = time.time()\n",
    "    final_average100 = run(interaction_time = 10000,reset_time = 300,eps_fixed=False, end_exploration_time = 3e5, min_eps=0.01, eval_every=300, eval_runs=1, worker=1,optimal_state=[0,0,0])\n",
    "    t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff46b69-4287-44bb-b28b-fc81ef3ff989",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (3, 4)\n",
    "tensor = torch.empty(size)  # Creates a tensor of shape (3, 4) with default dtype and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afe266a2-04cd-4a7b-bf65-7e59cb076356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: [0.40467017 0.76137428 1.02188813]\n",
      "Action size: 3\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "Reward: 0\n",
      "State size: (101, 3)\n",
      "Reward: -0.0015857025152846748\n",
      "State size: (101, 3)\n",
      "Reward: -0.743593837503503\n",
      "State size: (101, 3)\n",
      "Reward: -0.7956726877257926\n",
      "State size: (101, 3)\n",
      "Reward: -0.010319655195537764\n",
      "State size: (101, 3)\n",
      "Reward: -0.6118851650935891\n",
      "State size: (101, 3)\n",
      "Reward: -0.8965590297839583\n",
      "State size: (101, 3)\n",
      "Reward: -0.05806126340881416\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "env = Env()\n",
    "state = env.reset()\n",
    "print(f\"Initial state: {state}\")\n",
    "\n",
    "action = [(1, 1), (-1, 0.5), (1, 2)]  # Example action\n",
    "\n",
    "# Calculate and print action size\n",
    "action_size = len(action)\n",
    "print(f\"Action size: {action_size}\")\n",
    "\n",
    "# Simulate a few steps until we have 100 points\n",
    "for _ in range(105):  # We loop more times to ensure we pass the 100 points threshold\n",
    "    next_state, reward = env.step(action)\n",
    "    if isinstance(next_state, np.ndarray):  # Ensure next_state is ready\n",
    "        state_size = next_state.shape\n",
    "        print(f\"State size: {state_size}\")\n",
    "    print(f\"Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45328222-18d0-4d5c-8c19-91fac240a58b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf79c683-9ad4-4df2-bfbb-0280f1e8688b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
